# Flask/llama_client.py
"""
Client Python pour communiquer avec le serveur Llama.cpp
G√®re la g√©n√©ration de rapports d'analyse g√©opolitique
"""

import logging
import requests
from typing import Dict, List, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class LlamaClient:
    """Client pour interagir avec llama.cpp server"""
    
    def __init__(self, endpoint: str = "http://localhost:8080"):
        self.endpoint = endpoint
        self.timeout = 180  # 3 minutes pour √™tre large
        
        # Templates de prompts par type de rapport
        self.prompt_templates = {
            'geopolitique': self._build_geopolitique_prompt,
            'economique': self._build_economique_prompt,
            'securite': self._build_securite_prompt,
            'synthese': self._build_synthese_prompt
        }
    
    def test_connection(self) -> bool:
        """Teste la connexion au serveur Llama"""
        try:
            response = requests.get(
                f"{self.endpoint}/health",
                timeout=5
            )
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Connexion Llama √©chou√©e: {e}")
            return False
    
    def _build_geopolitique_prompt(self, articles: List[Dict], 
                                   context: Dict) -> str:
        """Construit le prompt pour analyse g√©opolitique"""
        
        # Pr√©parer le r√©sum√© des donn√©es
        sentiment_summary = f"""
Positifs: {context.get('sentiment_positive', 0)} articles
N√©gatifs: {context.get('sentiment_negative', 0)} articles
Neutres: {context.get('sentiment_neutral', 0)} articles
"""
        
        # Extraire les titres les plus pertinents
        top_articles = "\n".join([
            f"- {art['title']} ({art.get('source', 'source inconnue')})"
            for art in articles[:10]
        ])
        
        themes_covered = context.get('themes', [])
        themes_text = ", ".join(themes_covered) if themes_covered else "Tous th√®mes"
        
        prompt = f"""Tu es GEOPOL, un expert en analyse g√©opolitique reconnu. Tu dois produire un rapport professionnel structur√© et factuel.

CONTEXTE DE L'ANALYSE
======================
P√©riode: {context.get('period', 'Non sp√©cifi√©e')}
Nombre d'articles: {len(articles)}
Th√®mes couverts: {themes_text}

DISTRIBUTION DES SENTIMENTS
============================
{sentiment_summary}

ARTICLES PRINCIPAUX
===================
{top_articles}

RAPPORT G√âOPOLITIQUE DEMAND√â
=============================

Produis un rapport structur√© en 4 sections :

## 1. SYNTH√àSE EX√âCUTIVE (2-3 phrases)
R√©sum√© des tendances majeures observ√©es

## 2. ANALYSE DES TENDANCES
- Identifier 3-4 tendances g√©opolitiques principales
- Pour chaque tendance : contexte, acteurs, implications
- Utiliser un langage professionnel et pr√©cis

## 3. POINTS DE TENSION D√âTECT√âS
- Signaler les zones de conflit ou tension croissante
- Expliquer les causes sous-jacentes
- √âvaluer le niveau de risque (faible/moyen/√©lev√©)

## 4. PERSPECTIVES ET RECOMMANDATIONS
- Sc√©narios probables √† court terme (1-3 mois)
- Actions de veille recommand√©es
- Indicateurs √† surveiller

INSTRUCTIONS CRITIQUES :
- Sois factuel et nuanc√©, √©vite les g√©n√©ralisations
- Base-toi UNIQUEMENT sur les articles fournis
- Utilise un ton professionnel adapt√© √† un briefing strat√©gique
- Cite les sources pertinentes quand n√©cessaire
- Longueur cible : 800-1200 mots

Commence directement par "## 1. SYNTH√àSE EX√âCUTIVE" sans pr√©ambule.
"""
        return prompt
    
    def _build_economique_prompt(self, articles: List[Dict], 
                                 context: Dict) -> str:
        """Construit le prompt pour analyse √©conomique"""
        
        top_articles = "\n".join([
            f"- {art['title']}"
            for art in articles[:10]
        ])
        
        prompt = f"""Tu es un analyste √©conomique senior sp√©cialis√© en macro√©conomie. Produis une analyse structur√©e.

DONN√âES √Ä ANALYSER
==================
P√©riode: {context.get('period', 'Non sp√©cifi√©e')}
Articles analys√©s: {len(articles)}

TITRES CL√âS
===========
{top_articles}

RAPPORT √âCONOMIQUE DEMAND√â
===========================

## 1. INDICATEURS MACRO√âCONOMIQUES
- R√©sumer les tendances √©conomiques principales (croissance, inflation, march√©s)
- Identifier les secteurs en mouvement

## 2. POLITIQUES √âCONOMIQUES
- Analyser les d√©cisions politiques majeures
- Impact sur les march√©s et l'√©conomie r√©elle
- R√©ponse des acteurs √©conomiques

## 3. RISQUES ET OPPORTUNIT√âS
- Identifier les risques syst√©miques
- Signaler les opportunit√©s d'investissement
- Recommandations strat√©giques

## 4. PR√âVISIONS √Ä COURT TERME
- Sc√©narios probables (3-6 mois)
- Facteurs de volatilit√© √† surveiller

Base-toi UNIQUEMENT sur les articles fournis. Longueur : 600-900 mots.
Commence par "## 1. INDICATEURS MACRO√âCONOMIQUES".
"""
        return prompt
    
    def _build_securite_prompt(self, articles: List[Dict], 
                               context: Dict) -> str:
        """Construit le prompt pour analyse s√©curit√©"""
        
        top_articles = "\n".join([
            f"- {art['title']}"
            for art in articles[:8]
        ])
        
        prompt = f"""Tu es un expert en s√©curit√© internationale et analyse des menaces. Produis un briefing s√©curitaire.

CONTEXTE
========
P√©riode: {context.get('period', 'Non sp√©cifi√©e')}
Articles: {len(articles)}

√âV√âNEMENTS CL√âS
===============
{top_articles}

BRIEFING S√âCURITAIRE
====================

## 1. MENACES √âMERGENTES
- Identifier les nouvelles menaces ou escalades
- Qualifier le niveau de risque (critique/√©lev√©/mod√©r√©)

## 2. ACTEURS ET DYNAMIQUES
- Cartographier les acteurs impliqu√©s (√âtats, groupes)
- Analyser les rapports de force

## 3. IMPLICATIONS R√âGIONALES
- Impact sur la stabilit√© r√©gionale
- Risques de contagion

## 4. RECOMMANDATIONS OP√âRATIONNELLES
- Mesures de vigilance √† adopter
- Zones √† surveiller prioritairement

Ton professionnel et factuel. 500-800 mots.
Commence par "## 1. MENACES √âMERGENTES".
"""
        return prompt
    
    def _build_synthese_prompt(self, articles: List[Dict], 
                               context: Dict) -> str:
        """Construit le prompt pour synth√®se hebdomadaire"""
        
        top_articles = "\n".join([
            f"- {art['title']}"
            for art in articles[:15]
        ])
        
        prompt = f"""Tu es GEOPOL, sp√©cialiste en synth√®se d'actualit√© internationale. Produis une synth√®se hebdomadaire.

P√âRIODE COUVERTE
================
{context.get('period', 'Derni√®re semaine')}
{len(articles)} articles analys√©s

ARTICLES MAJEURS
================
{top_articles}

SYNTH√àSE HEBDOMADAIRE
=====================

## 1. FAITS MARQUANTS
- R√©sumer les 5 √©v√©nements majeurs de la semaine
- Une phrase par √©v√©nement, factuelle

## 2. TENDANCES OBSERV√âES
- Identifier 3 tendances significatives
- Expliquer leur importance strat√©gique

## 3. √âVOLUTIONS G√âOPOLITIQUES
- Changements dans les √©quilibres de pouvoir
- Nouvelles alliances ou tensions

## 4. AGENDA DE LA SEMAINE √Ä VENIR
- √âv√©nements √† surveiller
- √âch√©ances importantes

Style concis et informatif. 600-900 mots.
Commence par "## 1. FAITS MARQUANTS".
"""
        return prompt
    
    def generate_analysis(self, report_type: str, articles: List[Dict],
                         context: Dict) -> Dict:
        """
        G√©n√®re une analyse avec Llama
        
        Args:
            report_type: Type de rapport (geopolitique, economique, etc.)
            articles: Liste d'articles √† analyser
            context: Contexte additionnel (p√©riode, th√®mes, etc.)
            
        Returns:
            Dict avec 'success', 'analysis' et √©ventuellement 'error'
        """
        
        # V√©rifier la connexion
        if not self.test_connection():
            logger.warning("‚ö†Ô∏è Serveur Llama inaccessible - mode d√©grad√©")
            return {
                'success': False,
                'error': 'Serveur Llama inaccessible',
                'analysis': self._generate_fallback_analysis(
                    report_type, articles, context
                )
            }
        
        try:
            # Construire le prompt
            prompt_builder = self.prompt_templates.get(
                report_type, 
                self._build_geopolitique_prompt
            )
            prompt = prompt_builder(articles, context)
            
            logger.info(f"ü¶ô Envoi prompt √† Llama ({len(prompt)} caract√®res)")
            
            # Construction du prompt ChatML complet
            full_prompt = f"""<|im_start|>system
Tu es un analyste g√©opolitique professionnel. Ta mission est d'analyser des articles de presse et de produire des rapports structur√©s. Tu ne fais jamais de commentaires sur tes capacit√©s, tu analyses directement les donn√©es fournies.<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
## SYNTH√àSE EX√âCUTIVE

"""
            
            # Appel au serveur Llama avec format optimis√©
            logger.info(f"üì§ Envoi requ√™te √† {self.endpoint}/completion")
            
            response = requests.post(
                f"{self.endpoint}/completion",
                json={
                    "prompt": full_prompt,
                    "temperature": 0.7,
                    "max_tokens": 2000,
                    "stop": ["<|im_end|>", "<|im_start|>", "user:", "assistant:"],
                    "stream": False
                },
                headers={
                    "Content-Type": "application/json"
                },
                timeout=self.timeout
            )
            
            logger.info(f"üì• R√©ponse HTTP: {response.status_code}")
            
            if response.status_code != 200:
                logger.error(f"Contenu erreur: {response.text[:500]}")
                raise Exception(f"Erreur serveur Llama: {response.status_code}")
            
            data = response.json()
            logger.debug(f"Cl√©s JSON re√ßues: {list(data.keys())}")
            
            # Extraire la r√©ponse (format /completion)
            analysis_text = data.get('content', '').strip()
            
            # Nettoyer les balises ChatML r√©siduelles
            analysis_text = analysis_text.replace('<|im_start|>', '').replace('<|im_end|>', '')
            analysis_text = analysis_text.replace('assistant:', '').replace('user:', '')
            analysis_text = analysis_text.strip()
            
            # Si le contenu commence par le prompt system/user, extraire seulement la r√©ponse
            if '<|im_start|>assistant' in analysis_text:
                # Extraire tout ce qui suit le marqueur assistant
                parts = analysis_text.split('<|im_start|>assistant')
                if len(parts) > 1:
                    analysis_text = parts[-1].split('<|im_end|>')[0].strip()
            
            if not analysis_text:
                raise Exception("R√©ponse vide de Llama")
            
            # V√©rification souple : seulement si TOUTE la r√©ponse est le prompt
            if len(analysis_text) < 100 and ("Tu es" in analysis_text or "CONTEXTE" in analysis_text):
                raise Exception("R√©ponse trop courte ou invalide")
            
            logger.info(f"‚úÖ Analyse g√©n√©r√©e ({len(analysis_text)} caract√®res)")
            logger.debug(f"D√©but de l'analyse: {analysis_text[:200]}")
            
            return {
                'success': True,
                'analysis': analysis_text,
                'model_used': 'llama3.2-3b-Q4_K_M',
                'prompt_tokens': len(prompt.split()),
                'completion_tokens': len(analysis_text.split())
            }
            
        except requests.Timeout:
            logger.error("‚è±Ô∏è Timeout Llama - mode d√©grad√©")
            return {
                'success': False,
                'error': 'Timeout - analyse trop longue',
                'analysis': self._generate_fallback_analysis(
                    report_type, articles, context
                )
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Llama: {e}")
            return {
                'success': False,
                'error': str(e),
                'analysis': self._generate_fallback_analysis(
                    report_type, articles, context
                )
            }
    
    def _generate_fallback_analysis(self, report_type: str, 
                                    articles: List[Dict],
                                    context: Dict) -> str:
        """
        G√©n√®re une analyse de secours (mode d√©grad√©)
        """
        
        sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
        for article in articles:
            sentiment = article.get('sentiment', 'neutral')
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
        
        # Identifier les sources principales
        sources = {}
        for article in articles:
            source = article.get('source', 'Source inconnue')
            sources[source] = sources.get(source, 0) + 1
        
        top_sources = sorted(sources.items(), key=lambda x: x[1], reverse=True)[:3]
        
        analysis = f"""
## RAPPORT {report_type.upper()} - MODE D√âGRAD√â

**Note importante:** Ce rapport a √©t√© g√©n√©r√© en mode d√©grad√© (serveur IA indisponible). L'analyse est limit√©e aux statistiques descriptives.

### üìä Vue d'ensemble

**P√©riode analys√©e:** {context.get('period', 'Non sp√©cifi√©e')}  
**Articles trait√©s:** {len(articles)}  
**Th√®mes couverts:** {', '.join(context.get('themes', ['Tous th√®mes']))}

### üìà Distribution des sentiments

- **Positifs:** {sentiment_counts['positive']} articles ({sentiment_counts['positive']/len(articles)*100:.1f}%)
- **N√©gatifs:** {sentiment_counts['negative']} articles ({sentiment_counts['negative']/len(articles)*100:.1f}%)
- **Neutres:** {sentiment_counts['neutral']} articles ({sentiment_counts['neutral']/len(articles)*100:.1f}%)

### üì∞ Sources principales

{chr(10).join([f'{i+1}. {source} ({count} articles)' for i, (source, count) in enumerate(top_sources)])}

### üìã Articles significatifs

{chr(10).join([f'**{i+1}.** {article["title"]}' for i, article in enumerate(articles[:5])])}

### ‚ö†Ô∏è Limitations

Cette analyse automatique ne remplace pas l'expertise humaine. Pour une analyse approfondie avec IA :

1. V√©rifiez que le serveur Llama est d√©marr√© : `http://localhost:8080/health`
2. Relancez la g√©n√©ration du rapport
3. Ou consultez les articles individuellement pour une analyse manuelle

---
*G√©n√©r√© par GEOPOL Analytics - {datetime.now().strftime('%d/%m/%Y √† %H:%M')}*
"""
        
        return analysis


# Instance globale (singleton)
_llama_client = None

def get_llama_client() -> LlamaClient:
    """Retourne l'instance singleton du client Llama"""
    global _llama_client
    if _llama_client is None:
        _llama_client = LlamaClient()
    return _llama_client
