# Flask/llama_client.py - VERSION CORRIG√âE
"""
Client Python pour communiquer avec le serveur Llama.cpp
G√®re la g√©n√©ration de rapports d'analyse g√©opolitique
"""

import logging
import requests
from typing import Dict, List, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class LlamaClient:
    """Client pour interagir avec llama.cpp server"""
    
    def __init__(self, endpoint: str = "http://localhost:8080"):
        self.endpoint = endpoint
        self.timeout = 180  # 3 minutes
        
        # Templates de prompts par type de rapport
        self.prompt_templates = {
            'geopolitique': self._build_geopolitique_prompt,
            'economique': self._build_economique_prompt,
            'securite': self._build_securite_prompt,
            'synthese': self._build_synthese_prompt
        }
    
    def test_connection(self) -> bool:
        """Teste la connexion au serveur Llama"""
        try:
            # ‚ö†Ô∏è CORRECTION : Llama.cpp n'a pas de /health, tester avec /v1/models
            response = requests.get(
                f"{self.endpoint}/v1/models",
                timeout=5
            )
            return response.status_code == 200
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Connexion Llama √©chou√©e: {e}")
            return False
    
    def _build_geopolitique_prompt(self, articles: List[Dict], context: Dict) -> str:
        """Construit le prompt pour analyse g√©opolitique"""
        
        sentiment_summary = f"""
Positifs: {context.get('sentiment_positive', 0)} articles
N√©gatifs: {context.get('sentiment_negative', 0)} articles
Neutres: {context.get('sentiment_neutral', 0)} articles
"""
        
        top_articles = "\n".join([
            f"- {art['title']}"
            for art in articles[:8]
        ])
        
        themes_covered = context.get('themes', [])
        themes_text = ", ".join(themes_covered) if themes_covered else "Tous th√®mes"
        
        prompt = f"""Analyse g√©opolitique professionnelle

CONTEXTE
========
P√©riode: {context.get('period', 'Non sp√©cifi√©e')}
Articles analys√©s: {len(articles)}
Th√®mes: {themes_text}

SENTIMENTS
==========
{sentiment_summary}

TITRES PRINCIPAUX
=================
{top_articles}

CONSIGNE
========
Produis un rapport structur√© en 4 sections COURTES (200 mots maximum par section) :

1. SYNTH√àSE EX√âCUTIVE
- 3 tendances majeures en 1-2 phrases chacune

2. POINTS CL√âS
- 3-4 faits marquants
- Contexte minimal

3. TENSIONS
- Zones de conflit identifi√©es
- Niveau de risque (faible/moyen/√©lev√©)

4. PERSPECTIVES
- Sc√©narios probables (1-3 mois)
- 2-3 indicateurs √† surveiller

IMP√âRATIF: Sois concis, factuel et professionnel. Base-toi UNIQUEMENT sur les titres fournis.
Commence DIRECTEMENT par "## 1. SYNTH√àSE EX√âCUTIVE" sans introduction.
"""
        return prompt
    
    def _build_economique_prompt(self, articles: List[Dict], context: Dict) -> str:
        """Construit le prompt pour analyse √©conomique"""
        
        top_articles = "\n".join([f"- {art['title']}" for art in articles[:8]])
        
        prompt = f"""Analyse √©conomique

CONTEXTE
========
P√©riode: {context.get('period', 'Non sp√©cifi√©e')}
Articles: {len(articles)}

TITRES
======
{top_articles}

RAPPORT (4 sections courtes)
============================

1. INDICATEURS MACRO√âCONOMIQUES
- Tendances principales (croissance, inflation, march√©s)
- Secteurs en mouvement

2. POLITIQUES √âCONOMIQUES
- D√©cisions majeures
- Impact sur les march√©s

3. RISQUES ET OPPORTUNIT√âS
- Risques syst√©miques identifi√©s
- Opportunit√©s d'investissement

4. PR√âVISIONS (3-6 mois)
- Sc√©narios probables
- Facteurs de volatilit√©

Commence par "## 1. INDICATEURS MACRO√âCONOMIQUES". 600 mots maximum.
"""
        return prompt
    
    def _build_securite_prompt(self, articles: List[Dict], context: Dict) -> str:
        """Construit le prompt pour analyse s√©curit√©"""
        
        top_articles = "\n".join([f"- {art['title']}" for art in articles[:8]])
        
        prompt = f"""Briefing s√©curit√©

CONTEXTE
========
P√©riode: {context.get('period', 'Non sp√©cifi√©e')}
Articles: {len(articles)}

√âV√âNEMENTS
==========
{top_articles}

BRIEFING (4 sections)
====================

1. MENACES √âMERGENTES
- Nouvelles menaces ou escalades
- Niveau de risque

2. ACTEURS ET DYNAMIQUES
- Acteurs impliqu√©s (√âtats, groupes)
- Rapports de force

3. IMPLICATIONS R√âGIONALES
- Impact sur la stabilit√©
- Risques de contagion

4. RECOMMANDATIONS
- Mesures de vigilance
- Zones √† surveiller

Commence par "## 1. MENACES √âMERGENTES". 500 mots maximum.
"""
        return prompt
    
    def _build_synthese_prompt(self, articles: List[Dict], context: Dict) -> str:
        """Construit le prompt pour synth√®se hebdomadaire"""
        
        top_articles = "\n".join([f"- {art['title']}" for art in articles[:12]])
        
        prompt = f"""Synth√®se hebdomadaire

P√âRIODE
=======
{context.get('period', 'Derni√®re semaine')}
{len(articles)} articles

ARTICLES
========
{top_articles}

SYNTH√àSE (4 sections)
====================

1. FAITS MARQUANTS
- 5 √©v√©nements majeurs (1 phrase chacun)

2. TENDANCES
- 3 tendances significatives
- Importance strat√©gique

3. √âVOLUTIONS G√âOPOLITIQUES
- Changements dans les √©quilibres de pouvoir
- Nouvelles alliances ou tensions

4. AGENDA SEMAINE √Ä VENIR
- √âv√©nements √† surveiller
- √âch√©ances importantes

Commence par "## 1. FAITS MARQUANTS". 600 mots maximum.
"""
        return prompt
    
    def generate_analysis(self, report_type: str, articles: List[Dict],
                         context: Dict) -> Dict:
        """
        G√©n√®re une analyse avec Llama
        
        Args:
            report_type: Type de rapport (geopolitique, economique, etc.)
            articles: Liste d'articles √† analyser
            context: Contexte additionnel (p√©riode, th√®mes, etc.)
            
        Returns:
            Dict avec 'success', 'analysis' et √©ventuellement 'error'
        """
        
        # V√©rifier la connexion
        if not self.test_connection():
            logger.warning("‚ö†Ô∏è Serveur Llama inaccessible - mode d√©grad√©")
            return {
                'success': False,
                'error': 'Serveur Llama inaccessible sur ' + self.endpoint,
                'analysis': self._generate_fallback_analysis(
                    report_type, articles, context
                )
            }
        
        try:
            # Construire le prompt
            prompt_builder = self.prompt_templates.get(
                report_type, 
                self._build_geopolitique_prompt
            )
            prompt = prompt_builder(articles, context)
            
            logger.info(f"ü¶ô Envoi prompt √† Llama ({len(prompt)} caract√®res)")
            
            # üîß CORRECTION : Utiliser /completion au lieu de /v1/chat/completions
            logger.info(f"üì§ Envoi requ√™te √† {self.endpoint}/completion")
            
            response = requests.post(
                f"{self.endpoint}/completion",
                json={
                    "prompt": prompt,
                    "temperature": 0.7,
                    "top_k": 40,
                    "top_p": 0.9,
                    "n_predict": 1500,  # Nombre de tokens √† g√©n√©rer
                    "stop": ["##", "CONTEXTE", "CONSIGNE"],  # Arr√™ter si on r√©p√®te le prompt
                    "stream": False
                },
                headers={
                    "Content-Type": "application/json"
                },
                timeout=self.timeout
            )
            
            logger.info(f"üì• R√©ponse HTTP: {response.status_code}")
            
            if response.status_code != 200:
                logger.error(f"Contenu erreur: {response.text[:500]}")
                raise Exception(f"Erreur serveur Llama: {response.status_code}")
            
            data = response.json()
            logger.debug(f"Cl√©s JSON re√ßues: {list(data.keys())}")
            
            # Extraire la r√©ponse (format /completion)
            analysis_text = data.get('content', '').strip()
            
            if not analysis_text:
                raise Exception("R√©ponse vide de Llama")
            
            # Nettoyer les r√©p√©titions du prompt
            if "CONTEXTE" in analysis_text or "CONSIGNE" in analysis_text:
                # Ne garder que ce qui vient apr√®s le dernier "##"
                parts = analysis_text.split("##")
                if len(parts) > 1:
                    analysis_text = "## " + parts[-1].strip()
            
            # V√©rification : au moins 100 caract√®res
            if len(analysis_text) < 100:
                raise Exception("R√©ponse trop courte ou invalide")
            
            logger.info(f"‚úÖ Analyse g√©n√©r√©e ({len(analysis_text)} caract√®res)")
            
            return {
                'success': True,
                'analysis': analysis_text,
                'model_used': 'llama3.2-3b-Q4_K_M',
                'prompt_tokens': len(prompt.split()),
                'completion_tokens': len(analysis_text.split())
            }
            
        except requests.Timeout:
            logger.error("‚è±Ô∏è Timeout Llama - mode d√©grad√©")
            return {
                'success': False,
                'error': 'Timeout - analyse trop longue',
                'analysis': self._generate_fallback_analysis(
                    report_type, articles, context
                )
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Llama: {e}")
            return {
                'success': False,
                'error': str(e),
                'analysis': self._generate_fallback_analysis(
                    report_type, articles, context
                )
            }
    
    def _generate_fallback_analysis(self, report_type: str, 
                                    articles: List[Dict],
                                    context: Dict) -> str:
        """
        G√©n√®re une analyse de secours (mode d√©grad√©)
        """
        
        sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
        for article in articles:
            sentiment = article.get('sentiment', 'neutral')
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
        
        # Identifier les sources principales
        sources = {}
        for article in articles:
            source = article.get('source', 'Source inconnue')
            sources[source] = sources.get(source, 0) + 1
        
        top_sources = sorted(sources.items(), key=lambda x: x[1], reverse=True)[:3]
        
        analysis = f"""
## RAPPORT {report_type.upper()} - MODE D√âGRAD√â

**Note:** Ce rapport a √©t√© g√©n√©r√© en mode d√©grad√© (serveur IA indisponible). L'analyse est limit√©e aux statistiques descriptives.

### üìä Vue d'ensemble

**P√©riode analys√©e:** {context.get('period', 'Non sp√©cifi√©e')}  
**Articles trait√©s:** {len(articles)}  
**Th√®mes couverts:** {', '.join(context.get('themes', ['Tous th√®mes']))}

### üìà Distribution des sentiments

- **Positifs:** {sentiment_counts['positive']} articles ({sentiment_counts['positive']/len(articles)*100:.1f}%)
- **N√©gatifs:** {sentiment_counts['negative']} articles ({sentiment_counts['negative']/len(articles)*100:.1f}%)
- **Neutres:** {sentiment_counts['neutral']} articles ({sentiment_counts['neutral']/len(articles)*100:.1f}%)

### üì∞ Sources principales

{chr(10).join([f'{i+1}. {source} ({count} articles)' for i, (source, count) in enumerate(top_sources)])}

### üìã Articles significatifs

{chr(10).join([f'**{i+1}.** {article["title"]}' for i, article in enumerate(articles[:5])])}

### ‚ö†Ô∏è Limitations

Cette analyse automatique ne remplace pas l'expertise humaine. Pour une analyse approfondie avec IA :

1. V√©rifiez que le serveur Llama est d√©marr√© : `{self.endpoint}`
2. Testez avec : `curl {self.endpoint}/v1/models`
3. Relancez la g√©n√©ration du rapport

---
*G√©n√©r√© par GEOPOL Analytics - {datetime.now().strftime('%d/%m/%Y √† %H:%M')}*
"""
        
        return analysis


# Instance globale (singleton)
_llama_client = None

def get_llama_client() -> LlamaClient:
    """Retourne l'instance singleton du client Llama"""
    global _llama_client
    if _llama_client is None:
        _llama_client = LlamaClient()
    return _llama_client